{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq_Morpheme.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0ASIUZ-iNXc",
        "colab_type": "code",
        "outputId": "76b6d3f9-0ff5-4f85-f7ac-bec01721bab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 50  # Number of epochs to train for.\n",
        "latent_dim = 512  # Latent dimensionality of the encoding space.\n",
        "num_samples = 68617  # Number of samples\n",
        "# Path to the data txt file on disk.\n",
        "#data_path = 'fra.txt'\n",
        "data_path = 'MorphoLEX_en_shuffle.tsv'\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "cnt = 0\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    #input_text, target_text, _ = line.split('\\t')\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    new_target = \"\"\n",
        "    blank = False\n",
        "    for char in target_text:\n",
        "      if (char.isalpha()):\n",
        "        if (blank):\n",
        "          new_target += ' '\n",
        "          blank = False\n",
        "        new_target += char\n",
        "      elif (new_target != \"\"):\n",
        "        blank = True\n",
        "    target_text = '\\t' + new_target + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    if (cnt >= 10000): # skip test data\n",
        "      for char in input_text:\n",
        "          if char not in input_characters:\n",
        "              input_characters.add(char)\n",
        "      for char in target_text:\n",
        "          if char not in target_characters:\n",
        "              target_characters.add(char)\n",
        "    cnt += 1\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "\n",
        "# split test data\n",
        "from sklearn.model_selection import train_test_split\n",
        "input_texts, test_input, target_texts, test_target = train_test_split(\n",
        "    input_texts, target_texts, test_size=5000, random_state=837)\n",
        "\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n",
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    #encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
        "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.1)\n",
        "# Save model\n",
        "model.save('s2s.h5')\n",
        "\n",
        "# Next: inference mode (sampling).\n",
        "# Here's the drill:\n",
        "# 1) encode input and retrieve initial decoder state\n",
        "# 2) run one step of decoder with this initial state\n",
        "# and a \"start of sequence\" token as target.\n",
        "# Output will be the next target token\n",
        "# 3) Repeat with the current target token and current states\n",
        "\n",
        "# Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "# example of traning data\n",
        "for seq_index in range(100):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)\n",
        "    print('Target sentence:', target_texts[seq_index])    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 63617\n",
            "Number of unique input tokens: 31\n",
            "Number of unique output tokens: 29\n",
            "Max sequence length for inputs: 22\n",
            "Max sequence length for outputs: 27\n",
            "Train on 57255 samples, validate on 6362 samples\n",
            "Epoch 1/50\n",
            "57255/57255 [==============================] - 78s 1ms/step - loss: 0.7451 - accuracy: 0.7809 - val_loss: 0.4162 - val_accuracy: 0.8700\n",
            "Epoch 2/50\n",
            "57255/57255 [==============================] - 76s 1ms/step - loss: 0.3233 - accuracy: 0.8976 - val_loss: 0.2692 - val_accuracy: 0.9132\n",
            "Epoch 3/50\n",
            "57255/57255 [==============================] - 76s 1ms/step - loss: 0.1901 - accuracy: 0.9391 - val_loss: 0.1568 - val_accuracy: 0.9494\n",
            "Epoch 4/50\n",
            "57255/57255 [==============================] - 76s 1ms/step - loss: 0.1161 - accuracy: 0.9626 - val_loss: 0.1003 - val_accuracy: 0.9680\n",
            "Epoch 5/50\n",
            "57255/57255 [==============================] - 76s 1ms/step - loss: 0.0775 - accuracy: 0.9751 - val_loss: 0.0873 - val_accuracy: 0.9719\n",
            "Epoch 6/50\n",
            "57255/57255 [==============================] - 76s 1ms/step - loss: 0.0551 - accuracy: 0.9823 - val_loss: 0.0706 - val_accuracy: 0.9767\n",
            "Epoch 7/50\n",
            "57255/57255 [==============================] - 76s 1ms/step - loss: 0.0412 - accuracy: 0.9867 - val_loss: 0.0572 - val_accuracy: 0.9818\n",
            "Epoch 8/50\n",
            "57255/57255 [==============================] - 76s 1ms/step - loss: 0.0314 - accuracy: 0.9899 - val_loss: 0.0530 - val_accuracy: 0.9834\n",
            "Epoch 9/50\n",
            "57255/57255 [==============================] - 76s 1ms/step - loss: 0.0246 - accuracy: 0.9922 - val_loss: 0.0443 - val_accuracy: 0.9865\n",
            "Epoch 10/50\n",
            "57255/57255 [==============================] - 76s 1ms/step - loss: 0.0194 - accuracy: 0.9938 - val_loss: 0.0430 - val_accuracy: 0.9874\n",
            "Epoch 11/50\n",
            "57255/57255 [==============================] - 76s 1ms/step - loss: 0.0160 - accuracy: 0.9949 - val_loss: 0.0401 - val_accuracy: 0.9882\n",
            "Epoch 12/50\n",
            "57255/57255 [==============================] - 78s 1ms/step - loss: 0.0136 - accuracy: 0.9958 - val_loss: 0.0389 - val_accuracy: 0.9890\n",
            "Epoch 13/50\n",
            "57255/57255 [==============================] - 79s 1ms/step - loss: 0.0117 - accuracy: 0.9964 - val_loss: 0.0363 - val_accuracy: 0.9896\n",
            "Epoch 14/50\n",
            "57255/57255 [==============================] - 81s 1ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0368 - val_accuracy: 0.9900\n",
            "Epoch 15/50\n",
            "57255/57255 [==============================] - 81s 1ms/step - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0349 - val_accuracy: 0.9913\n",
            "Epoch 16/50\n",
            "57255/57255 [==============================] - 81s 1ms/step - loss: 0.0078 - accuracy: 0.9976 - val_loss: 0.0346 - val_accuracy: 0.9913\n",
            "Epoch 17/50\n",
            "57255/57255 [==============================] - 79s 1ms/step - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.0348 - val_accuracy: 0.9910\n",
            "Epoch 18/50\n",
            "57255/57255 [==============================] - 80s 1ms/step - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.0360 - val_accuracy: 0.9911\n",
            "Epoch 19/50\n",
            "57255/57255 [==============================] - 81s 1ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.0330 - val_accuracy: 0.9918\n",
            "Epoch 20/50\n",
            "57255/57255 [==============================] - 81s 1ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0336 - val_accuracy: 0.9918\n",
            "Epoch 21/50\n",
            "57255/57255 [==============================] - 80s 1ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.0356 - val_accuracy: 0.9914\n",
            "Epoch 22/50\n",
            "57255/57255 [==============================] - 81s 1ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.0363 - val_accuracy: 0.9915\n",
            "Epoch 23/50\n",
            "57255/57255 [==============================] - 81s 1ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.0328 - val_accuracy: 0.9925\n",
            "Epoch 24/50\n",
            "57255/57255 [==============================] - 81s 1ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0335 - val_accuracy: 0.9924\n",
            "Epoch 25/50\n",
            "57255/57255 [==============================] - 82s 1ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0341 - val_accuracy: 0.9921\n",
            "Epoch 26/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0350 - val_accuracy: 0.9922\n",
            "Epoch 27/50\n",
            "57255/57255 [==============================] - 82s 1ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.0341 - val_accuracy: 0.9925\n",
            "Epoch 28/50\n",
            "57255/57255 [==============================] - 82s 1ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.0343 - val_accuracy: 0.9925\n",
            "Epoch 29/50\n",
            "57255/57255 [==============================] - 82s 1ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0359 - val_accuracy: 0.9920\n",
            "Epoch 30/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0350 - val_accuracy: 0.9926\n",
            "Epoch 31/50\n",
            "57255/57255 [==============================] - 82s 1ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0361 - val_accuracy: 0.9923\n",
            "Epoch 32/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0340 - val_accuracy: 0.9928\n",
            "Epoch 33/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0340 - val_accuracy: 0.9927\n",
            "Epoch 34/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0334 - val_accuracy: 0.9929\n",
            "Epoch 35/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0343 - val_accuracy: 0.9928\n",
            "Epoch 36/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0357 - val_accuracy: 0.9929\n",
            "Epoch 37/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0341 - val_accuracy: 0.9933\n",
            "Epoch 38/50\n",
            "57255/57255 [==============================] - 84s 1ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0347 - val_accuracy: 0.9929\n",
            "Epoch 39/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0359 - val_accuracy: 0.9928\n",
            "Epoch 40/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0345 - val_accuracy: 0.9933\n",
            "Epoch 41/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0365 - val_accuracy: 0.9928\n",
            "Epoch 42/50\n",
            "57255/57255 [==============================] - 84s 1ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0355 - val_accuracy: 0.9931\n",
            "Epoch 43/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0365 - val_accuracy: 0.9929\n",
            "Epoch 44/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0356 - val_accuracy: 0.9931\n",
            "Epoch 45/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0344 - val_accuracy: 0.9934\n",
            "Epoch 46/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0356 - val_accuracy: 0.9929\n",
            "Epoch 47/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0356 - val_accuracy: 0.9932\n",
            "Epoch 48/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0346 - val_accuracy: 0.9932\n",
            "Epoch 49/50\n",
            "57255/57255 [==============================] - 84s 1ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0359 - val_accuracy: 0.9930\n",
            "Epoch 50/50\n",
            "57255/57255 [==============================] - 83s 1ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0362 - val_accuracy: 0.9933\n",
            "-\n",
            "Input sentence: frightening\n",
            "Decoded sentence: fright en\n",
            "\n",
            "Target sentence: \tfright en\n",
            "\n",
            "-\n",
            "Input sentence: prostrations\n",
            "Decoded sentence: prostrate ion\n",
            "\n",
            "Target sentence: \tprostrate ion\n",
            "\n",
            "-\n",
            "Input sentence: libertines\n",
            "Decoded sentence: liber ine\n",
            "\n",
            "Target sentence: \tliber ine\n",
            "\n",
            "-\n",
            "Input sentence: numbing\n",
            "Decoded sentence: numb\n",
            "\n",
            "Target sentence: \tnumb\n",
            "\n",
            "-\n",
            "Input sentence: inflame\n",
            "Decoded sentence: in flame\n",
            "\n",
            "Target sentence: \tin flame\n",
            "\n",
            "-\n",
            "Input sentence: antenatal\n",
            "Decoded sentence: ante natal\n",
            "\n",
            "Target sentence: \tante natal\n",
            "\n",
            "-\n",
            "Input sentence: illnesses\n",
            "Decoded sentence: ill ness\n",
            "\n",
            "Target sentence: \till ness\n",
            "\n",
            "-\n",
            "Input sentence: commentator\n",
            "Decoded sentence: comment ate or\n",
            "\n",
            "Target sentence: \tcomment ate or\n",
            "\n",
            "-\n",
            "Input sentence: thirsts\n",
            "Decoded sentence: thirst\n",
            "\n",
            "Target sentence: \tthirst\n",
            "\n",
            "-\n",
            "Input sentence: fettle\n",
            "Decoded sentence: fettle\n",
            "\n",
            "Target sentence: \tfettle\n",
            "\n",
            "-\n",
            "Input sentence: hallucinatory\n",
            "Decoded sentence: hallucin ate ory\n",
            "\n",
            "Target sentence: \thallucin ate ory\n",
            "\n",
            "-\n",
            "Input sentence: referendums\n",
            "Decoded sentence: referendum\n",
            "\n",
            "Target sentence: \treferendum\n",
            "\n",
            "-\n",
            "Input sentence: infiltrations\n",
            "Decoded sentence: in filter ate ion\n",
            "\n",
            "Target sentence: \tin filter ate ion\n",
            "\n",
            "-\n",
            "Input sentence: quitely\n",
            "Decoded sentence: quit ely\n",
            "\n",
            "Target sentence: \tquit ely\n",
            "\n",
            "-\n",
            "Input sentence: appreciably\n",
            "Decoded sentence: a preci able y\n",
            "\n",
            "Target sentence: \ta preci able y\n",
            "\n",
            "-\n",
            "Input sentence: stonewalled\n",
            "Decoded sentence: stone wall\n",
            "\n",
            "Target sentence: \tstone wall\n",
            "\n",
            "-\n",
            "Input sentence: spoons\n",
            "Decoded sentence: spoon\n",
            "\n",
            "Target sentence: \tspoon\n",
            "\n",
            "-\n",
            "Input sentence: elderberries\n",
            "Decoded sentence: elder berry\n",
            "\n",
            "Target sentence: \telder berry\n",
            "\n",
            "-\n",
            "Input sentence: polka\n",
            "Decoded sentence: polka\n",
            "\n",
            "Target sentence: \tpolka\n",
            "\n",
            "-\n",
            "Input sentence: disestablishing\n",
            "Decoded sentence: dis establish\n",
            "\n",
            "Target sentence: \tdis establish\n",
            "\n",
            "-\n",
            "Input sentence: muting\n",
            "Decoded sentence: mute\n",
            "\n",
            "Target sentence: \tmute\n",
            "\n",
            "-\n",
            "Input sentence: convinced\n",
            "Decoded sentence: convince\n",
            "\n",
            "Target sentence: \tconvince\n",
            "\n",
            "-\n",
            "Input sentence: flagstaff\n",
            "Decoded sentence: flag staff\n",
            "\n",
            "Target sentence: \tflag staff\n",
            "\n",
            "-\n",
            "Input sentence: tearless\n",
            "Decoded sentence: tear less\n",
            "\n",
            "Target sentence: \ttear less\n",
            "\n",
            "-\n",
            "Input sentence: squaller\n",
            "Decoded sentence: squall er\n",
            "\n",
            "Target sentence: \tsquall er\n",
            "\n",
            "-\n",
            "Input sentence: stable\n",
            "Decoded sentence: stable\n",
            "\n",
            "Target sentence: \tstable\n",
            "\n",
            "-\n",
            "Input sentence: assortment\n",
            "Decoded sentence: a sort ment\n",
            "\n",
            "Target sentence: \ta sort ment\n",
            "\n",
            "-\n",
            "Input sentence: recto\n",
            "Decoded sentence: rect\n",
            "\n",
            "Target sentence: \trect\n",
            "\n",
            "-\n",
            "Input sentence: contextually\n",
            "Decoded sentence: co text al y\n",
            "\n",
            "Target sentence: \tco text al y\n",
            "\n",
            "-\n",
            "Input sentence: rubberizing\n",
            "Decoded sentence: rub er ize\n",
            "\n",
            "Target sentence: \trub er ize\n",
            "\n",
            "-\n",
            "Input sentence: respect\n",
            "Decoded sentence: respect\n",
            "\n",
            "Target sentence: \trespect\n",
            "\n",
            "-\n",
            "Input sentence: letters\n",
            "Decoded sentence: letter\n",
            "\n",
            "Target sentence: \tletter\n",
            "\n",
            "-\n",
            "Input sentence: brooking\n",
            "Decoded sentence: brook\n",
            "\n",
            "Target sentence: \tbrook\n",
            "\n",
            "-\n",
            "Input sentence: plaintive\n",
            "Decoded sentence: plaint ive\n",
            "\n",
            "Target sentence: \tplaint ive\n",
            "\n",
            "-\n",
            "Input sentence: bailiff\n",
            "Decoded sentence: bailiff\n",
            "\n",
            "Target sentence: \tbailiff\n",
            "\n",
            "-\n",
            "Input sentence: gigantic\n",
            "Decoded sentence: gigantic\n",
            "\n",
            "Target sentence: \tgigantic\n",
            "\n",
            "-\n",
            "Input sentence: showplace\n",
            "Decoded sentence: show place\n",
            "\n",
            "Target sentence: \tshow place\n",
            "\n",
            "-\n",
            "Input sentence: skittles\n",
            "Decoded sentence: skittle\n",
            "\n",
            "Target sentence: \tskittle\n",
            "\n",
            "-\n",
            "Input sentence: syncopates\n",
            "Decoded sentence: syncopate\n",
            "\n",
            "Target sentence: \tsyncopate\n",
            "\n",
            "-\n",
            "Input sentence: fluently\n",
            "Decoded sentence: flu ant ly\n",
            "\n",
            "Target sentence: \tflu ant ly\n",
            "\n",
            "-\n",
            "Input sentence: readiest\n",
            "Decoded sentence: ready est\n",
            "\n",
            "Target sentence: \tready est\n",
            "\n",
            "-\n",
            "Input sentence: bantered\n",
            "Decoded sentence: banter\n",
            "\n",
            "Target sentence: \tbanter\n",
            "\n",
            "-\n",
            "Input sentence: peri\n",
            "Decoded sentence: peri\n",
            "\n",
            "Target sentence: \tperi\n",
            "\n",
            "-\n",
            "Input sentence: pryingly\n",
            "Decoded sentence: pry ly\n",
            "\n",
            "Target sentence: \tpry ly\n",
            "\n",
            "-\n",
            "Input sentence: braiding\n",
            "Decoded sentence: braid\n",
            "\n",
            "Target sentence: \tbraid\n",
            "\n",
            "-\n",
            "Input sentence: vulcanized\n",
            "Decoded sentence: vulcan ize\n",
            "\n",
            "Target sentence: \tvulcan ize\n",
            "\n",
            "-\n",
            "Input sentence: enfold\n",
            "Decoded sentence: en fold\n",
            "\n",
            "Target sentence: \ten fold\n",
            "\n",
            "-\n",
            "Input sentence: scribbles\n",
            "Decoded sentence: scribble\n",
            "\n",
            "Target sentence: \tscribble\n",
            "\n",
            "-\n",
            "Input sentence: shirtsleeve\n",
            "Decoded sentence: shirt sleeve\n",
            "\n",
            "Target sentence: \tshirt sleeve\n",
            "\n",
            "-\n",
            "Input sentence: beneficiary\n",
            "Decoded sentence: beneficiary\n",
            "\n",
            "Target sentence: \tbeneficiary\n",
            "\n",
            "-\n",
            "Input sentence: resembling\n",
            "Decoded sentence: re semble\n",
            "\n",
            "Target sentence: \tre semble\n",
            "\n",
            "-\n",
            "Input sentence: skyjackings\n",
            "Decoded sentence: sky jack\n",
            "\n",
            "Target sentence: \tsky jack\n",
            "\n",
            "-\n",
            "Input sentence: hurdlers\n",
            "Decoded sentence: hurdle er\n",
            "\n",
            "Target sentence: \thurdle er\n",
            "\n",
            "-\n",
            "Input sentence: besmears\n",
            "Decoded sentence: be smear\n",
            "\n",
            "Target sentence: \tbe smear\n",
            "\n",
            "-\n",
            "Input sentence: bodyguards\n",
            "Decoded sentence: body guard\n",
            "\n",
            "Target sentence: \tbody guard\n",
            "\n",
            "-\n",
            "Input sentence: gimlets\n",
            "Decoded sentence: gimlet\n",
            "\n",
            "Target sentence: \tgimlet\n",
            "\n",
            "-\n",
            "Input sentence: mann's\n",
            "Decoded sentence: mann\n",
            "\n",
            "Target sentence: \tmann\n",
            "\n",
            "-\n",
            "Input sentence: intercede\n",
            "Decoded sentence: inter cede\n",
            "\n",
            "Target sentence: \tinter cede\n",
            "\n",
            "-\n",
            "Input sentence: greatness\n",
            "Decoded sentence: great ness\n",
            "\n",
            "Target sentence: \tgreat ness\n",
            "\n",
            "-\n",
            "Input sentence: lodges\n",
            "Decoded sentence: lodge\n",
            "\n",
            "Target sentence: \tlodge\n",
            "\n",
            "-\n",
            "Input sentence: nippy\n",
            "Decoded sentence: nip y\n",
            "\n",
            "Target sentence: \tnip y\n",
            "\n",
            "-\n",
            "Input sentence: elasticities\n",
            "Decoded sentence: elastic ity\n",
            "\n",
            "Target sentence: \telastic ity\n",
            "\n",
            "-\n",
            "Input sentence: curiae\n",
            "Decoded sentence: curiae\n",
            "\n",
            "Target sentence: \tcuriae\n",
            "\n",
            "-\n",
            "Input sentence: taskmaster\n",
            "Decoded sentence: task master\n",
            "\n",
            "Target sentence: \ttask master\n",
            "\n",
            "-\n",
            "Input sentence: christie\n",
            "Decoded sentence: christie\n",
            "\n",
            "Target sentence: \tchristie\n",
            "\n",
            "-\n",
            "Input sentence: drills\n",
            "Decoded sentence: drill\n",
            "\n",
            "Target sentence: \tdrill\n",
            "\n",
            "-\n",
            "Input sentence: enzymatic\n",
            "Decoded sentence: enzyme ic\n",
            "\n",
            "Target sentence: \tenzyme ic\n",
            "\n",
            "-\n",
            "Input sentence: cutthroats\n",
            "Decoded sentence: cut throat\n",
            "\n",
            "Target sentence: \tcut throat\n",
            "\n",
            "-\n",
            "Input sentence: slipshod\n",
            "Decoded sentence: slip shod\n",
            "\n",
            "Target sentence: \tslip shod\n",
            "\n",
            "-\n",
            "Input sentence: explosive\n",
            "Decoded sentence: ex plode ive\n",
            "\n",
            "Target sentence: \tex plode ive\n",
            "\n",
            "-\n",
            "Input sentence: decomposes\n",
            "Decoded sentence: de co pose\n",
            "\n",
            "Target sentence: \tde co pose\n",
            "\n",
            "-\n",
            "Input sentence: discontenting\n",
            "Decoded sentence: dis content\n",
            "\n",
            "Target sentence: \tdis content\n",
            "\n",
            "-\n",
            "Input sentence: tenacity\n",
            "Decoded sentence: ten ity\n",
            "\n",
            "Target sentence: \tten ity\n",
            "\n",
            "-\n",
            "Input sentence: thereto\n",
            "Decoded sentence: there to\n",
            "\n",
            "Target sentence: \tthere to\n",
            "\n",
            "-\n",
            "Input sentence: prodigally\n",
            "Decoded sentence: prodig al ly\n",
            "\n",
            "Target sentence: \tprodig al ly\n",
            "\n",
            "-\n",
            "Input sentence: wades\n",
            "Decoded sentence: wade\n",
            "\n",
            "Target sentence: \twade\n",
            "\n",
            "-\n",
            "Input sentence: diplomats\n",
            "Decoded sentence: diploma t\n",
            "\n",
            "Target sentence: \tdiploma t\n",
            "\n",
            "-\n",
            "Input sentence: impropriety\n",
            "Decoded sentence: im propriety\n",
            "\n",
            "Target sentence: \tim propriety\n",
            "\n",
            "-\n",
            "Input sentence: decorative\n",
            "Decoded sentence: decor ate ive\n",
            "\n",
            "Target sentence: \tdecor ate ive\n",
            "\n",
            "-\n",
            "Input sentence: frescoed\n",
            "Decoded sentence: fresco\n",
            "\n",
            "Target sentence: \tfresco\n",
            "\n",
            "-\n",
            "Input sentence: zinc\n",
            "Decoded sentence: zinc\n",
            "\n",
            "Target sentence: \tzinc\n",
            "\n",
            "-\n",
            "Input sentence: slightness\n",
            "Decoded sentence: slight ness\n",
            "\n",
            "Target sentence: \tslight ness\n",
            "\n",
            "-\n",
            "Input sentence: utopian\n",
            "Decoded sentence: u top ia n\n",
            "\n",
            "Target sentence: \tu top ia n\n",
            "\n",
            "-\n",
            "Input sentence: clairvoyant\n",
            "Decoded sentence: clairvoyant\n",
            "\n",
            "Target sentence: \tclairvoyant\n",
            "\n",
            "-\n",
            "Input sentence: recharges\n",
            "Decoded sentence: re charge\n",
            "\n",
            "Target sentence: \tre charge\n",
            "\n",
            "-\n",
            "Input sentence: nutriment\n",
            "Decoded sentence: nutri ment\n",
            "\n",
            "Target sentence: \tnutri ment\n",
            "\n",
            "-\n",
            "Input sentence: implanting\n",
            "Decoded sentence: in plant\n",
            "\n",
            "Target sentence: \tin plant\n",
            "\n",
            "-\n",
            "Input sentence: winch\n",
            "Decoded sentence: winch\n",
            "\n",
            "Target sentence: \twinch\n",
            "\n",
            "-\n",
            "Input sentence: dividend\n",
            "Decoded sentence: divide end\n",
            "\n",
            "Target sentence: \tdivide end\n",
            "\n",
            "-\n",
            "Input sentence: busby\n",
            "Decoded sentence: busby\n",
            "\n",
            "Target sentence: \tbusby\n",
            "\n",
            "-\n",
            "Input sentence: distillation\n",
            "Decoded sentence: distill ion\n",
            "\n",
            "Target sentence: \tdistill ion\n",
            "\n",
            "-\n",
            "Input sentence: leg\n",
            "Decoded sentence: leg\n",
            "\n",
            "Target sentence: \tleg\n",
            "\n",
            "-\n",
            "Input sentence: place\n",
            "Decoded sentence: place\n",
            "\n",
            "Target sentence: \tplace\n",
            "\n",
            "-\n",
            "Input sentence: prep\n",
            "Decoded sentence: prep\n",
            "\n",
            "Target sentence: \tprep\n",
            "\n",
            "-\n",
            "Input sentence: grenade\n",
            "Decoded sentence: grenade\n",
            "\n",
            "Target sentence: \tgrenade\n",
            "\n",
            "-\n",
            "Input sentence: seemliness\n",
            "Decoded sentence: seemly ness\n",
            "\n",
            "Target sentence: \tseemly ness\n",
            "\n",
            "-\n",
            "Input sentence: boutflower\n",
            "Decoded sentence: bout flower\n",
            "\n",
            "Target sentence: \tbout flower\n",
            "\n",
            "-\n",
            "Input sentence: neutral\n",
            "Decoded sentence: neutr al\n",
            "\n",
            "Target sentence: \tneutr al\n",
            "\n",
            "-\n",
            "Input sentence: yeti\n",
            "Decoded sentence: yeti\n",
            "\n",
            "Target sentence: \tyeti\n",
            "\n",
            "-\n",
            "Input sentence: indecorum\n",
            "Decoded sentence: im decor ium\n",
            "\n",
            "Target sentence: \tim decor ium\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJautJbG_eGI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "059fabfb-52a4-40b3-d286-409a69488668"
      },
      "source": [
        "# test model\n",
        "test_input_data = np.zeros(\n",
        "    (len(test_input), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "for i, (input_text, target_text) in enumerate(zip(test_input, test_target)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        test_input_data[i, t, input_token_index[char]] = 1.\n",
        "\n",
        "correct = 0\n",
        "for seq_index in range(len(test_input)):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = test_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    if (decoded_sentence.strip() == test_target[seq_index].strip()):\n",
        "      correct += 1\n",
        "    else:\n",
        "      print('Incorrect: ', test_input[seq_index], '---', decoded_sentence.strip(), '---', test_target[seq_index].strip())\n",
        "print(len(test_input), correct, '  Accuracy:', correct/len(test_input)*100.0)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Incorrect:  locals --- loc al --- local\n",
            "Incorrect:  anabaptists --- anabaptist --- ana bapt ist\n",
            "Incorrect:  demagnification --- demangic ion --- de magn ify ion\n",
            "Incorrect:  theatrically --- theatric ly --- theater ic al ly\n",
            "Incorrect:  alluvial --- alluvia al --- alluvial\n",
            "Incorrect:  toxins --- toxicn --- toxico in\n",
            "Incorrect:  butting --- but --- butt\n",
            "Incorrect:  faience --- faicena --- faience\n",
            "Incorrect:  conscientiousness --- conscience ious ness --- conscientious ness\n",
            "Incorrect:  limpets --- limp et --- limpet\n",
            "Incorrect:  protract --- pro tract ure --- pro tract\n",
            "Incorrect:  lawns --- law n --- lawn\n",
            "Incorrect:  resoluteness --- resolutene --- resolute ness\n",
            "Incorrect:  manlier --- manlier --- man ly er\n",
            "Incorrect:  chrysalis --- chryslain --- chrysalis\n",
            "Incorrect:  antipodes --- antipode --- antipodes\n",
            "Incorrect:  inconsistencies --- im co sist ancy --- im co sist ance y\n",
            "Incorrect:  absolutism --- absoluti ism --- absolute ism\n",
            "Incorrect:  papaw --- pap aw --- papaw\n",
            "Incorrect:  castorbeans --- castorbean --- castor bean\n",
            "Incorrect:  pharmacological --- pharma log ic al ly --- pharma log ic al\n",
            "Incorrect:  boasts --- boa st --- boast\n",
            "Incorrect:  shriven --- shriven --- shrive en\n",
            "Incorrect:  schubert --- schube er --- schubert\n",
            "Incorrect:  psychokinetically --- psycho lint ic ly --- psycho kine ic ly\n",
            "Incorrect:  insolvent --- insolvent --- im solve ant\n",
            "Incorrect:  relaid --- relaid --- re lay\n",
            "Incorrect:  subornation --- sub ornino ion --- suborn ion\n",
            "Incorrect:  harelipped --- hare lipp --- hare lip\n",
            "Incorrect:  chinese --- chine see --- china ese\n",
            "Incorrect:  baroque --- bar ious --- baroque\n",
            "Incorrect:  anniversary --- anni verse ory --- anniversary\n",
            "Incorrect:  habitually --- habit al ly --- habit al y\n",
            "Incorrect:  mayhem --- may hem --- mayhem\n",
            "Incorrect:  paginates --- pagenate --- page ate\n",
            "Incorrect:  pittsburgh --- pit butger --- pittsburgh\n",
            "Incorrect:  encephalitis --- en cephal ity --- en cephalo itis\n",
            "Incorrect:  lucratively --- lucrate ive ly --- lucre ive ly\n",
            "Incorrect:  doctrinaire --- doctrine ory --- doctrine aire\n",
            "Incorrect:  hindus --- hindus --- hindu\n",
            "Incorrect:  amphibious --- amphibious --- amphibio ious\n",
            "Incorrect:  sentient --- sentient --- sent ant\n",
            "Incorrect:  jammy --- jamy --- jam y\n",
            "Incorrect:  dynamism --- dynaminm --- dynamism\n",
            "Incorrect:  glummer --- glummer --- glum er\n",
            "Incorrect:  idiosyncrasies --- idio syn crass --- idio syn crasy\n",
            "Incorrect:  campaigners --- campaignes --- campaign er\n",
            "Incorrect:  backgammon --- back gam mon --- back gammon\n",
            "Incorrect:  vibrancy --- vibr ancy --- vibr ance y\n",
            "Incorrect:  seerey --- seer ey --- see rey\n",
            "Incorrect:  antiphonal --- anti phon al --- antiphonal\n",
            "Incorrect:  antediluvian --- ante dulic ian --- ante diluvian\n",
            "Incorrect:  birefringence --- bi rifgern --- bi refringence\n",
            "Incorrect:  thematic --- them ic --- theme ic\n",
            "Incorrect:  wardrobes --- wardrobe --- ward robe\n",
            "Incorrect:  mississippi --- missid ist ic --- mississippi\n",
            "Incorrect:  enroled --- en role --- enrol\n",
            "Incorrect:  superciliously --- super cilious ly --- super cili ious ly\n",
            "Incorrect:  hays --- hay --- hays\n",
            "Incorrect:  brontosaurus --- brontosaura --- bronto saur us\n",
            "Incorrect:  suborbital --- sub orbit able --- sub orbit al\n",
            "Incorrect:  prestige --- pre stige --- prestige\n",
            "Incorrect:  aforethought --- afor teoh ious --- afore thought\n",
            "Incorrect:  mesenteric --- mesenter ic --- mesenteric\n",
            "Incorrect:  pfennig --- pfenn --- pfennig\n",
            "Incorrect:  murkland --- murkland --- murk land\n",
            "Incorrect:  injuriously --- injure ious ly --- injure y ious ly\n",
            "Incorrect:  hollywood --- holly wood --- hollywood\n",
            "Incorrect:  girlie --- girly --- girl y\n",
            "Incorrect:  winsome --- win some --- winsome\n",
            "Incorrect:  funny --- funny --- fun y\n",
            "Incorrect:  crepes --- crepes --- crepe\n",
            "Incorrect:  trenchant --- trenchant --- trench ant\n",
            "Incorrect:  hungarian --- hungaria n --- hungary ian\n",
            "Incorrect:  pathless --- pat hless --- patho less\n",
            "Incorrect:  diplomacy --- diploma ic --- diploma acy\n",
            "Incorrect:  hawaiian --- haway ian --- hawaii ian\n",
            "Incorrect:  shouldst --- should est --- shouldst\n",
            "Incorrect:  nonflammable --- non flam able --- non flame able\n",
            "Incorrect:  weightiness --- weigh t ine --- weigh t y ness\n",
            "Incorrect:  magisterial --- magister ial --- magister al\n",
            "Incorrect:  foreseen --- fore sen --- fore see n\n",
            "Incorrect:  maddened --- mad men --- mad en\n",
            "Incorrect:  scribed --- scride --- script\n",
            "Incorrect:  goldberg --- gold er --- goldberg\n",
            "Incorrect:  photosensitive --- photo sense it ize --- photo sense ive\n",
            "Incorrect:  dude --- dud --- dude\n",
            "Incorrect:  delimitations --- de limit ion --- de limit ate ion\n",
            "Incorrect:  dormer --- dorm er --- dormer\n",
            "Incorrect:  prehensile --- prehensy an --- prehens ile\n",
            "Incorrect:  mucous --- muce ious --- mucous\n",
            "Incorrect:  treeless --- treel ess --- tree less\n",
            "Incorrect:  kaleidoscopic --- kaleido scope ic al --- kaleido scope ic\n",
            "Incorrect:  angostura --- angostar --- angostura\n",
            "Incorrect:  nonconformists --- non co from ist --- non co form ist\n",
            "Incorrect:  defter --- defter --- deft er\n",
            "Incorrect:  iowa --- ioowa --- iowa\n",
            "Incorrect:  avoirdupois --- avor id pous --- avoirdupois\n",
            "Incorrect:  inconvertible --- im convert able --- im co vert able\n",
            "Incorrect:  propagation --- propage ate ion --- propag ate ion\n",
            "Incorrect:  greenwood --- green wood --- greenwood\n",
            "Incorrect:  articling --- articling --- article\n",
            "Incorrect:  inclosed --- in clode --- im close\n",
            "Incorrect:  diurnal --- diurn al --- diurnal\n",
            "Incorrect:  kegham --- kegham --- keg ham\n",
            "Incorrect:  hoydenish --- hoydenish --- hoyden ish\n",
            "Incorrect:  coastguardsmen --- coast guard man --- coast guard men\n",
            "Incorrect:  protozoa --- proto zoa n --- proto zoa\n",
            "Incorrect:  eugenically --- eu gen ic al ly --- eu gen ic ly\n",
            "Incorrect:  sabring --- sabring --- sabre\n",
            "Incorrect:  krautheads --- krauthead --- kraut head\n",
            "Incorrect:  unprecedentedly --- un precedent al ly --- un precedent ly\n",
            "Incorrect:  sues --- sue --- su\n",
            "Incorrect:  effect --- ef fect --- e fect\n",
            "Incorrect:  fowlingpiece --- fowlingu ic --- fowl piece\n",
            "Incorrect:  peninsular --- peninsular --- peninsula ar\n",
            "Incorrect:  homemade --- home made --- home make\n",
            "Incorrect:  carbondale --- carbon ale --- carbondale\n",
            "Incorrect:  cunard --- cun ard --- cunard\n",
            "Incorrect:  swollenness --- swoll en ness --- swollen ness\n",
            "Incorrect:  interestedness --- inter est en --- interest ness\n",
            "Incorrect:  virility --- virility --- virile ity\n",
            "Incorrect:  drumbeat --- drumb eta --- drum beat\n",
            "Incorrect:  chestier --- chestier --- chest y er\n",
            "Incorrect:  runes --- rune --- run\n",
            "Incorrect:  apposition --- a pose it ion --- apposite ion\n",
            "Incorrect:  funnily --- funny ly --- fun y ly\n",
            "Incorrect:  goatee --- goat ee --- goatee\n",
            "Incorrect:  extramarital --- extra mart al --- extra marital\n",
            "Incorrect:  episodically --- epi doc ic al ly --- episode ic al ly\n",
            "Incorrect:  interpositions --- inter pose ion --- inter posit ion\n",
            "Incorrect:  cinderella --- cinder lela --- cinder ella\n",
            "Incorrect:  reappraisals --- re appraise lan --- re appraise al\n",
            "Incorrect:  retied --- retie --- re tie\n",
            "Incorrect:  contractually --- contract al ly --- contract al y\n",
            "Incorrect:  flatirons --- flaterion --- flat iron\n",
            "Incorrect:  blarney --- blance y --- blarney\n",
            "Incorrect:  tunisians --- tunis ian --- tunis ia n\n",
            "Incorrect:  wharves --- wharve --- wharf\n",
            "Incorrect:  boggs --- bog --- boggs\n",
            "Incorrect:  stripy --- strip y --- stripe y\n",
            "Incorrect:  detente --- detent --- detente\n",
            "Incorrect:  chrysalises --- chry salise --- chrysalis\n",
            "Incorrect:  don't --- don --- do\n",
            "Incorrect:  sanest --- sane ster --- sane est\n",
            "Incorrect:  tunisian --- tunis ian --- tunis ia n\n",
            "Incorrect:  hardwicke --- hard wick --- hardwicke\n",
            "Incorrect:  midsts --- mid st --- midst\n",
            "Incorrect:  tanker --- tanker --- tank er\n",
            "Incorrect:  divertingly --- divert ly --- di vert ly\n",
            "Incorrect:  delphiniums --- delp inumism --- delphinium\n",
            "Incorrect:  goobers --- goobe er --- goober\n",
            "Incorrect:  petits --- petit --- petits\n",
            "Incorrect:  skiver --- skiver --- skive er\n",
            "Incorrect:  sacramento --- sacro ment --- sacramento\n",
            "Incorrect:  aftertastes --- after state --- after taste\n",
            "Incorrect:  wagner --- wagn er --- wagner\n",
            "Incorrect:  gadgetry --- gadgerty --- gadget ory\n",
            "Incorrect:  reinvestigation --- re in vest ion --- re investigate ion\n",
            "Incorrect:  annals --- annal --- ann al\n",
            "Incorrect:  auditing --- audio in --- audio it\n",
            "Incorrect:  pasternak --- pasterna --- pasternak\n",
            "Incorrect:  gregg --- grigg --- gregg\n",
            "Incorrect:  mumford --- mum --- mumford\n",
            "Incorrect:  ottermole --- toter mole --- otter mole\n",
            "Incorrect:  odder --- odder --- odd er\n",
            "Incorrect:  outlawry --- out law ry --- out law ory\n",
            "Incorrect:  conductance --- conduct ance --- co duct ance\n",
            "Incorrect:  biodegradable --- biode grade able --- bio degrade able\n",
            "Incorrect:  spilt --- spilt --- spill t\n",
            "Incorrect:  stubblefields --- stubble y --- stubble field\n",
            "Incorrect:  flues --- flue --- flu\n",
            "Incorrect:  honduras --- hondur ia --- hondur as\n",
            "Incorrect:  samuels --- samuel --- samuels\n",
            "Incorrect:  expressionless --- ex press ion aes --- ex press ion less\n",
            "Incorrect:  petrie --- petry --- petrie\n",
            "Incorrect:  interdenominational --- inter demony ion al ist --- inter de nomin ate ion al\n",
            "Incorrect:  locket --- lock et --- locket\n",
            "Incorrect:  sealskin --- sealk sink --- seal skin\n",
            "Incorrect:  lichtenstein --- lichettinsent --- lichtenstein\n",
            "Incorrect:  romish --- romish --- rome ish\n",
            "Incorrect:  drumbeats --- drumb ate --- drum beat\n",
            "Incorrect:  quadruply --- quadr uply --- quadruple y\n",
            "Incorrect:  grippe --- grip --- grippe\n",
            "Incorrect:  curtis --- curti --- curtis\n",
            "Incorrect:  refinery --- refive or y --- refine ory\n",
            "Incorrect:  gallbladder --- galblladder --- gall bladder\n",
            "Incorrect:  impeded --- impede --- im ped\n",
            "Incorrect:  truer --- truer --- true er\n",
            "Incorrect:  methodically --- method ic ly --- method al ly\n",
            "Incorrect:  acoustics --- acoust ic --- acoust ics\n",
            "Incorrect:  mizzenmast --- mizzen maste --- mizzen mast\n",
            "Incorrect:  sexless --- sewlese --- sex less\n",
            "Incorrect:  paid --- paid --- pay\n",
            "Incorrect:  mangosteen --- mangoseten --- mangosteen\n",
            "Incorrect:  polyphony --- poly phon y --- poly phone y\n",
            "Incorrect:  fluent --- fluent --- flu ant\n",
            "Incorrect:  bellies --- bellie --- belly\n",
            "Incorrect:  mentally --- ment al ly --- mental ly\n",
            "Incorrect:  lacrosse --- lacross --- lacrosse\n",
            "Incorrect:  reasonableness --- re a nose able ness --- reason able ness\n",
            "Incorrect:  sadistic --- sadistic --- sadist ic\n",
            "Incorrect:  discomfited --- discomfite --- discomfit\n",
            "Incorrect:  grandniece --- grand neck --- grand niece\n",
            "Incorrect:  freebooters --- free booter --- free boot er\n",
            "Incorrect:  harcourt --- harcourt --- har court\n",
            "Incorrect:  transfuse --- trans fuse --- transfuse\n",
            "Incorrect:  dingdong --- dind gong --- ding dong\n",
            "Incorrect:  bullyboy --- bully housy --- bully boy\n",
            "Incorrect:  sugarloaves --- sugar lova --- sugar loaf\n",
            "Incorrect:  stoppable --- stop acle --- stop able\n",
            "Incorrect:  geddes --- ged er --- geddes\n",
            "Incorrect:  frontispiece --- frontis ipe y --- frontis piece\n",
            "Incorrect:  elliptically --- elliptic al ly --- elliptic ly\n",
            "Incorrect:  dyers --- dy er --- die er\n",
            "Incorrect:  contrariwise --- contra ory ize --- contra ory wise\n",
            "Incorrect:  insurance --- in surance --- in sure ance\n",
            "Incorrect:  handcuffing --- hand cuft --- hand cuff\n",
            "Incorrect:  brahmaputra --- brahmampat --- brahmaputra\n",
            "Incorrect:  beatnik --- bean tike --- beatnik\n",
            "Incorrect:  goodies --- goo die --- good y\n",
            "Incorrect:  chaos --- chao --- chaos\n",
            "Incorrect:  transatlantic --- transaltant ic --- trans atlantic\n",
            "Incorrect:  kleptomania --- klepto mania iac --- klepto mania\n",
            "Incorrect:  laredo --- lare do --- laredo\n",
            "Incorrect:  greasepaint --- grease pan ist --- grease paint\n",
            "Incorrect:  oratorical --- orate or al --- orate or ic al\n",
            "Incorrect:  crates --- crates --- crate\n",
            "Incorrect:  twanged --- twange --- twang\n",
            "Incorrect:  alchemists --- alchemist --- al chem ist\n",
            "Incorrect:  penumbra --- pen umbra --- penumbra\n",
            "Incorrect:  blumenthal --- blumenth al --- blumenthal\n",
            "Incorrect:  behemoths --- be hemoth --- behemoth\n",
            "Incorrect:  antepenultimate --- ante pencilate ion --- ante pen ultim ate\n",
            "Incorrect:  imbruing --- im bure --- imbrue\n",
            "Incorrect:  spidery --- spidery --- spider y\n",
            "Incorrect:  hippocratic --- hip porcit ac --- hippocrat ic\n",
            "Incorrect:  biotechnology --- biotech log y --- bio tech log y\n",
            "Incorrect:  vietnam --- viventam --- vietnam\n",
            "Incorrect:  treetops --- treet op --- tree top\n",
            "Incorrect:  countdown --- count wond --- count down\n",
            "Incorrect:  olde --- old --- olde\n",
            "Incorrect:  scholastica --- school astic a --- school astica\n",
            "Incorrect:  judson --- jud sono --- judson\n",
            "Incorrect:  bennington --- benning tone --- bennington\n",
            "Incorrect:  grimes --- grime --- grimes\n",
            "Incorrect:  prognostics --- pro gnose ic --- pro gnose tics\n",
            "Incorrect:  theatricals --- theatric al --- theater ic al\n",
            "Incorrect:  urinary --- urineray --- urine ory\n",
            "Incorrect:  intransigents --- in transig ant --- im transig ant\n",
            "Incorrect:  subparagraph --- sub pargar ly --- sub para graph\n",
            "Incorrect:  paschal --- pasche al --- paschal\n",
            "Incorrect:  drolly --- droll y --- droll ly\n",
            "Incorrect:  enunciations --- en unciate ion --- enunciate ion\n",
            "Incorrect:  kissers --- kisser --- kiss er\n",
            "Incorrect:  forensic --- forensic --- forens ic\n",
            "Incorrect:  pneumatically --- pneuma ic ly --- pneum ic ly\n",
            "Incorrect:  nautically --- nautic ly --- naut ic al ly\n",
            "Incorrect:  publick --- publick --- pube lick\n",
            "Incorrect:  keyless --- keyles --- key less\n",
            "Incorrect:  inappropriateness --- im a propri ate ment --- im a propri ate ness\n",
            "Incorrect:  hauteur --- hauteru --- hauteur\n",
            "Incorrect:  townships --- town stip --- town ship\n",
            "Incorrect:  surtaxing --- surtax --- sur tax\n",
            "Incorrect:  hosiery --- hose y y --- hosiery\n",
            "Incorrect:  icelandic --- icelandic --- iceland ic\n",
            "Incorrect:  testers --- tester --- test er\n",
            "Incorrect:  chairpersons --- chair prone --- chair person\n",
            "Incorrect:  gondolier --- gon dole y er --- gondol er\n",
            "Incorrect:  telecommunications --- teleco munic al ism --- tele communic ate ion\n",
            "Incorrect:  bodied --- body --- bodied\n",
            "Incorrect:  shillings --- shill --- shilling\n",
            "Incorrect:  hampshire --- hamp shire --- hampshire\n",
            "Incorrect:  pageantry --- pagenarty --- pageant ory\n",
            "Incorrect:  hums --- hum --- hume\n",
            "Incorrect:  collaborationists --- co labor ate ion ism --- co labor ate ion ist\n",
            "Incorrect:  danzig --- danze --- danzig\n",
            "Incorrect:  autoeroticism --- auto oryte ic --- auto eros ic ism\n",
            "Incorrect:  rawson --- raw sono --- rawson\n",
            "Incorrect:  dobbins --- dob in --- dobbins\n",
            "Incorrect:  bountiful --- bountiful --- bounty ful\n",
            "Incorrect:  ninepenny --- nine pen y ness --- nine penny\n",
            "Incorrect:  noninflammable --- non inflambina --- non in flame able\n",
            "Incorrect:  psychoactive --- psycho cate ic --- psycho act ive\n",
            "Incorrect:  robert --- roberts --- robert\n",
            "Incorrect:  genteelly --- genteel ly --- genteel y\n",
            "Incorrect:  antiquate --- antiquate --- antique ate\n",
            "Incorrect:  paroxysms --- paro yxsm --- paroxysm\n",
            "Incorrect:  ezra --- ezar --- ezra\n",
            "Incorrect:  megalomania --- megalo mania iac --- megalo mania\n",
            "Incorrect:  stanford --- stan ford --- stanford\n",
            "Incorrect:  claptrap --- claptrap --- clap trap\n",
            "Incorrect:  apoplexies --- apoplex ian --- apoplex y\n",
            "Incorrect:  incongruous --- incongru ious --- im congru ious\n",
            "Incorrect:  overaggressive --- over aggrest ion --- over aggress ive\n",
            "Incorrect:  facto --- fact o --- facto\n",
            "Incorrect:  johnnie --- johnnine --- johnnie\n",
            "Incorrect:  northampton --- north mapton --- northampton\n",
            "Incorrect:  helicopter --- helic poter --- helicopter\n",
            "Incorrect:  alongside --- along ide --- along side\n",
            "Incorrect:  nettlerash --- net lethar --- nettle rash\n",
            "Incorrect:  puttin --- puttin --- put tin\n",
            "Incorrect:  malaya --- malay a --- malaya\n",
            "Incorrect:  marilyn --- marily --- marilyn\n",
            "Incorrect:  prices --- prices --- price\n",
            "Incorrect:  mahout --- mahouth --- mahout\n",
            "Incorrect:  apocalyptic --- apocalyptic --- apocalypse ic\n",
            "Incorrect:  surveillance --- sur viel ance --- sur vey ance\n",
            "Incorrect:  myers --- my er --- myers\n",
            "Incorrect:  lobularity --- lobe al ory --- lobe ule ar ity\n",
            "Incorrect:  superciliousness --- super ci ious ness --- super cili ious ness\n",
            "Incorrect:  piquing --- piqu --- pique\n",
            "Incorrect:  mentholated --- mentholate --- menthol ate\n",
            "Incorrect:  pacification --- pacif ic ity --- pacif y ion\n",
            "Incorrect:  apostolically --- apostlic ly --- apostle ic ly\n",
            "Incorrect:  kandinsky --- kand ish ly --- kandinsky\n",
            "Incorrect:  dulcet --- duclet --- dulcet\n",
            "Incorrect:  existent --- existent --- exist ant\n",
            "Incorrect:  neocortex --- neo corte --- neo cortex\n",
            "Incorrect:  hattie --- hatite --- hattie\n",
            "Incorrect:  hostage --- hosteage --- hostage\n",
            "Incorrect:  anastomoses --- anastomos --- anastomoses\n",
            "Incorrect:  oxyacetylene --- oxoyach ety ly --- oxyacetylene\n",
            "Incorrect:  lacklustre --- lack sult er --- lack luster\n",
            "Incorrect:  unimpeachable --- un impech able --- un impeach able\n",
            "Incorrect:  dissertation --- disserattino --- dissertation\n",
            "Incorrect:  bronchi --- bronchi i --- bronchi\n",
            "Incorrect:  pneumatic --- pneuma ic --- pneum ic\n",
            "Incorrect:  withy --- with y --- withy\n",
            "Incorrect:  asking --- asking --- ask\n",
            "Incorrect:  bemaddening --- be maden --- be mad en\n",
            "Incorrect:  fibrin --- fibrin --- fiber in\n",
            "Incorrect:  bolshy --- bols hy --- bolshy\n",
            "Incorrect:  woburn --- woberund --- wo burn\n",
            "Incorrect:  waite --- waite --- wait\n",
            "Incorrect:  asher --- ash er --- asher\n",
            "Incorrect:  brookmont --- brook mont --- brookmont\n",
            "Incorrect:  behests --- be hest --- behest\n",
            "Incorrect:  juanita --- juane ate --- juanita\n",
            "Incorrect:  cannot --- cannot --- can not\n",
            "Incorrect:  awestricken --- awe strick --- awe stricken\n",
            "Incorrect:  bakelite --- bake lite --- bakelite\n",
            "Incorrect:  byres --- byres --- byre\n",
            "Incorrect:  former --- form er --- former\n",
            "Incorrect:  reservoirs --- reservo ir --- reservoir\n",
            "Incorrect:  aftertaste --- after state --- after taste\n",
            "Incorrect:  alias --- alia --- alias\n",
            "Incorrect:  lubberlanders --- lub lerd an ler --- lubber landers\n",
            "Incorrect:  skivers --- skiver --- skive er\n",
            "Incorrect:  chinaman --- chinaman --- china man\n",
            "Incorrect:  pathogenesis --- patho gen iss --- patho gen sis\n",
            "Incorrect:  drowsily --- drow sy ly --- drowse y ly\n",
            "Incorrect:  ultravehement --- ultra veem ness --- ultra vehem ant\n",
            "Incorrect:  crawford --- craw ford --- crawford\n",
            "Incorrect:  skyway --- sky way --- skyway\n",
            "Incorrect:  enos --- eno --- enos\n",
            "Incorrect:  tartaric --- tartaric --- tartar ic\n",
            "Incorrect:  coherency --- co here ancy --- co here ance y\n",
            "Incorrect:  jordan --- jor dan --- jordan\n",
            "Incorrect:  barstow --- bar stow --- barstow\n",
            "Incorrect:  southland --- south land --- southland\n",
            "Incorrect:  bilinear --- bilinear --- bi line ar\n",
            "Incorrect:  hayfields --- hay field --- hayfield\n",
            "Incorrect:  tarring --- tarr --- tar\n",
            "Incorrect:  viscid --- vis dic --- viscid\n",
            "Incorrect:  disastrous --- dis astrous --- disaster ous\n",
            "Incorrect:  mishitting --- mis hit --- mis hit ting\n",
            "Incorrect:  subatomic --- subatco ism --- sub atom ic\n",
            "Incorrect:  giorgio --- giorggio --- giorgio\n",
            "Incorrect:  noonday --- non day --- noon day\n",
            "Incorrect:  sponsorship --- sponsor ih --- sponsor ship\n",
            "Incorrect:  panicking --- panick --- panic\n",
            "Incorrect:  sempstresses --- semp stress --- sempstress\n",
            "Incorrect:  ultrasonic --- ultra son ic --- ultra sono ic\n",
            "Incorrect:  timesaving --- time saive --- time save\n",
            "Incorrect:  medicinal --- medicinal --- medicine al\n",
            "Incorrect:  whetted --- whett --- whet\n",
            "Incorrect:  manuscripts --- manuscript --- mani script\n",
            "Incorrect:  sachems --- sachem --- sac hem\n",
            "Incorrect:  amplitude --- amplitude --- ample itude\n",
            "Incorrect:  skied --- sky --- ski\n",
            "Incorrect:  revert --- revert --- re vert\n",
            "Incorrect:  chinning --- chin --- chinn\n",
            "Incorrect:  catarrh --- catarra --- catarrh\n",
            "Incorrect:  sundaes --- sun day --- sundae\n",
            "Incorrect:  singed --- sing --- singe\n",
            "Incorrect:  welfare --- welfare --- wel fare\n",
            "Incorrect:  ultrasonically --- ultra son ic al ly --- ultra sono ic ly\n",
            "Incorrect:  lizzie --- lizz y --- lizzie\n",
            "Incorrect:  inanimateness --- in anim ate ness --- im anim ate ness\n",
            "Incorrect:  accelerando --- a celer and --- a celer ando\n",
            "Incorrect:  nevada --- nev ada --- nevada\n",
            "Incorrect:  supercilious --- super clious --- super cili ious\n",
            "Incorrect:  germantown --- germantown --- german town\n",
            "Incorrect:  hiawatha --- hiagatha --- hiawatha\n",
            "Incorrect:  reynolds --- reyn lod --- reynolds\n",
            "Incorrect:  betake --- be take --- betake\n",
            "Incorrect:  burundi --- burnudin --- burundi\n",
            "Incorrect:  sextettes --- sexette --- sextette\n",
            "Incorrect:  manuscript --- man sucre it --- mani script\n",
            "Incorrect:  punitive --- pun ive ite --- pun ive\n",
            "Incorrect:  modalities --- modal ity --- mode al ity\n",
            "Incorrect:  interminable --- inter mint able --- im termin able\n",
            "Incorrect:  heretically --- heres ic ly --- heres ic al ly\n",
            "Incorrect:  pancreatic --- pancrete ic --- pancreatic\n",
            "Incorrect:  homozygous --- mooze gous --- homo zygo ious\n",
            "Incorrect:  countian --- countina --- count ian\n",
            "Incorrect:  exaggeratedly --- exaggeratello --- exaggerate ly\n",
            "Incorrect:  epidemiological --- epi demo log ic al --- epi demi log ic al\n",
            "Incorrect:  caudal --- caud al --- cauda al\n",
            "Incorrect:  toscanini --- toscanin --- toscanini\n",
            "Incorrect:  istanbul --- iston able --- istanbul\n",
            "Incorrect:  kentucky --- kent cumy --- kentucky\n",
            "Incorrect:  eleazar --- elea zar --- eleazar\n",
            "Incorrect:  impersonalized --- im person al ize --- in person al ize\n",
            "Incorrect:  ipso --- ip so --- ipso\n",
            "Incorrect:  poesy --- poese y --- poesy\n",
            "Incorrect:  tyburn --- tyburn --- ty burn\n",
            "Incorrect:  nestling --- nest --- nestle\n",
            "Incorrect:  eclectically --- eclectic al ly --- eclectic ly\n",
            "Incorrect:  soppy --- sopp y --- sop y\n",
            "Incorrect:  schwarzkopf --- schwark woid --- schwarzkopf\n",
            "Incorrect:  laundryman --- launder in may --- launder y man\n",
            "Incorrect:  implies --- implie --- imply\n",
            "Incorrect:  volkenstein --- volkent ize --- vol ken stein\n",
            "Incorrect:  yorkshire --- york shire --- yorkshire\n",
            "Incorrect:  fatboy --- fat ouy --- fat boy\n",
            "Incorrect:  semiweekly --- sime keep wy --- semi week ly\n",
            "Incorrect:  decoration --- decor ate ion --- decor ion\n",
            "Incorrect:  treasurer --- trease ure --- treasure er\n",
            "Incorrect:  derivative --- derive ive --- derive ate ive\n",
            "Incorrect:  todman --- todman --- tod man\n",
            "Incorrect:  cahill --- cahill --- ca hill\n",
            "Incorrect:  forsook --- fore sook --- forsook\n",
            "Incorrect:  malcolm --- mal colm --- malcolm\n",
            "Incorrect:  offprints --- off rint --- off print\n",
            "Incorrect:  freeport --- free port --- freeport\n",
            "Incorrect:  secretariat --- secretary --- secretariat\n",
            "Incorrect:  withheld --- withel --- with held\n",
            "Incorrect:  spoliation --- spoli ion --- spoil ion\n",
            "Incorrect:  lintels --- lint less --- lintel\n",
            "Incorrect:  terracotta --- terract or --- terracotta\n",
            "Incorrect:  entries --- entry --- enter y\n",
            "Incorrect:  ulceration --- ulcer ate ion --- ulcer ion\n",
            "Incorrect:  anabaptist --- anabaptist --- ana bapt ist\n",
            "Incorrect:  hydrofoil --- hydro fiol --- hydro foil\n",
            "Incorrect:  rudolph --- rudol phong --- rudolph\n",
            "Incorrect:  rime --- rim --- rime\n",
            "Incorrect:  stalinist --- stali in --- stalin ist\n",
            "Incorrect:  rosalie --- rose al y --- rosalie\n",
            "Incorrect:  pitt --- pitt --- pit\n",
            "Incorrect:  feasibly --- fease able y --- fact able y\n",
            "Incorrect:  gustation --- gustate ion --- gust ion\n",
            "Incorrect:  aqualungs --- aqual --- aqua lung\n",
            "Incorrect:  nichols --- nichol --- nichols\n",
            "Incorrect:  hillock --- hill oc --- hill ock\n",
            "Incorrect:  horseracing --- horser ace --- horse race\n",
            "Incorrect:  picnicker --- pictincker --- picnic er\n",
            "Incorrect:  utah --- ut ah --- utah\n",
            "Incorrect:  mavises --- mavise --- mavis\n",
            "Incorrect:  atavistic --- atave ist ic --- atavistic\n",
            "Incorrect:  agnosticism --- a gno ic ism --- a gnose ic ism\n",
            "Incorrect:  pungency --- pung en ic --- pungency\n",
            "Incorrect:  georgetown --- ge rog tent --- george town\n",
            "Incorrect:  juanita's --- juantia --- juanita\n",
            "Incorrect:  borne --- borne --- bore en\n",
            "Incorrect:  wendells --- wendell --- wen dell\n",
            "Incorrect:  goodbody --- goob dom y --- good body\n",
            "Incorrect:  prevaricated --- prevare arce --- prevaricate\n",
            "Incorrect:  intelligent --- intelligent --- intellig ant\n",
            "Incorrect:  disneyland --- dis neal --- disneyland\n",
            "Incorrect:  judy --- judy --- jud y\n",
            "Incorrect:  stragglier --- strag ly er --- straggle y er\n",
            "Incorrect:  ancestral --- ancestro al --- ancestor al\n",
            "Incorrect:  inkbottle --- in bittble --- ink bottle\n",
            "Incorrect:  ginghams --- ging ham --- gingham\n",
            "Incorrect:  brigadier --- brigadier --- brigade er\n",
            "Incorrect:  township --- town swip --- town ship\n",
            "Incorrect:  bunting --- bunt --- bunting\n",
            "Incorrect:  arduous --- arduo --- arduous\n",
            "Incorrect:  campaigner --- campaigner --- campaign er\n",
            "Incorrect:  gaieties --- gay iety --- gay ity\n",
            "Incorrect:  sanchez --- sanche --- sanchez\n",
            "Incorrect:  imps --- inp --- imp\n",
            "Incorrect:  circuitry --- circuiter y --- circuit ory\n",
            "Incorrect:  broadways --- broadway --- broad way\n",
            "Incorrect:  siegfried --- sieg fire --- siegfried\n",
            "Incorrect:  mosquitoes --- mosquitome --- mosquito\n",
            "Incorrect:  denier --- deny er --- denier\n",
            "Incorrect:  enmity --- enmitiy --- enmity\n",
            "Incorrect:  cotangent --- cotange ant --- co tang ant\n",
            "Incorrect:  roes --- roes --- roe\n",
            "Incorrect:  downstream --- down trema --- down stream\n",
            "Incorrect:  cicero --- cice ro --- cicero\n",
            "Incorrect:  grottoes --- grot oe --- grotto\n",
            "Incorrect:  pedantic --- pedantic --- pedant ic\n",
            "Incorrect:  abjectly --- ab eclth y --- abject ly\n",
            "Incorrect:  murphies --- murphy --- murphies\n",
            "Incorrect:  hemorrhoids --- hemorrohoid --- hema rrhoid\n",
            "Incorrect:  lapses --- lapsce --- lapse\n",
            "Incorrect:  goods --- good --- goods\n",
            "Incorrect:  citizenship --- citizen ly --- citizen ship\n",
            "Incorrect:  westerly --- wester ly --- west er ly\n",
            "Incorrect:  athletics --- athlete ic --- athlete ics\n",
            "Incorrect:  tallish --- tallish --- tall ish\n",
            "Incorrect:  nowise --- now ise --- no wise\n",
            "Incorrect:  fitzroy --- fit fox y --- fitzroy\n",
            "Incorrect:  predicator --- predic ate or --- pre dict ate or\n",
            "Incorrect:  hysterics --- hyster ics --- hyster ic\n",
            "Incorrect:  hippocrates --- hip porcate --- hippocrates\n",
            "Incorrect:  merrier --- merrier --- merry er\n",
            "Incorrect:  qataris --- qatar is --- qatar i\n",
            "Incorrect:  poop --- popo --- poop\n",
            "Incorrect:  grieved --- grive --- grieve\n",
            "Incorrect:  czechoslovakians --- czech slovan ifan --- czech slovak ia n\n",
            "Incorrect:  lintel --- lint le --- lintel\n",
            "Incorrect:  overtakin --- over take in --- over takin\n",
            "Incorrect:  socioeconomic --- socio colo ic ion --- socio econom ic\n",
            "Incorrect:  eisenhower --- eise hone worb --- eisenhower\n",
            "Incorrect:  surplus --- surplus --- sur plus\n",
            "Incorrect:  flannels --- flan less --- flannel\n",
            "Incorrect:  bribers --- briber --- bribe er\n",
            "Incorrect:  nonequivalence --- non equi val ant --- non equi val ance\n",
            "Incorrect:  animalcules --- animal cule --- animal ule\n",
            "Incorrect:  albs --- albs --- alb\n",
            "Incorrect:  clitorises --- clitory is --- clitoris\n",
            "Incorrect:  pupal --- pup al --- pup a al\n",
            "Incorrect:  epistemology --- epistemool ship --- epistem log y\n",
            "Incorrect:  quietuses --- quietusy --- quietus\n",
            "Incorrect:  mover --- mover --- move er\n",
            "Incorrect:  weevils --- weeve ild --- weevil\n",
            "Incorrect:  loonybins --- loon y ness --- loon y bin\n",
            "Incorrect:  roughshod --- rough soro --- rough shod\n",
            "Incorrect:  bales --- bales --- bale\n",
            "Incorrect:  ecclesiastics --- ecclesiast ic ant --- ecclesiast ic\n",
            "Incorrect:  brussels --- brussel --- brussels\n",
            "Incorrect:  patriotically --- pater ot ic al ly --- pater ot ic ly\n",
            "Incorrect:  undreamt --- undream t --- un dream t\n",
            "Incorrect:  icily --- icily --- ice y ly\n",
            "Incorrect:  stardom --- stardom --- star dom\n",
            "Incorrect:  interposition --- inter pose ion --- inter posit ion\n",
            "Incorrect:  urbanely --- urb an ly --- urbane ly\n",
            "Incorrect:  incredulity --- im cred al ity --- im credulity\n",
            "Incorrect:  gadget --- dagget --- gadget\n",
            "Incorrect:  tankard --- tandard --- tankard\n",
            "Incorrect:  antiredeposition --- antire depose ion --- anti re deposit ion\n",
            "Incorrect:  ophthalmoscopes --- ophthalmo schop --- ophthalmo scope\n",
            "Incorrect:  purloining --- purlonin --- purloin\n",
            "Incorrect:  montevideo --- monte vide --- monte video\n",
            "Incorrect:  hiroshima --- hiro shima --- hiroshima\n",
            "Incorrect:  palpations --- palp ate ion --- palp ion\n",
            "Incorrect:  volumetric --- volumetric --- volume etr ic\n",
            "Incorrect:  noncommittal --- non commit ate --- non commit al\n",
            "Incorrect:  impoliteness --- in polite ness --- im polite ness\n",
            "Incorrect:  knoxville --- tnx dovil --- knoxville\n",
            "Incorrect:  aspidistras --- aspidistrast --- aspidistra\n",
            "Incorrect:  fanned --- fann --- fan\n",
            "Incorrect:  buffoonery --- buffoon ery --- buffoon ory\n",
            "Incorrect:  deficiency --- defici ancy --- deficiency\n",
            "Incorrect:  addison --- add ion --- addison\n",
            "Incorrect:  waver --- wave er --- waver\n",
            "Incorrect:  scarface --- scarface --- scar face\n",
            "Incorrect:  barflies --- barflie --- bar fly\n",
            "Incorrect:  consensus --- consense --- consensus\n",
            "Incorrect:  rushall --- rushall --- rush all\n",
            "Incorrect:  sauerkraut --- sauer bark --- sauer kraut\n",
            "Incorrect:  gluttony --- gluttony --- glutton y\n",
            "Incorrect:  agricolas --- agri colas --- agricola\n",
            "Incorrect:  retinal --- retin al --- retina al\n",
            "Incorrect:  diabetes --- diabeta --- diabetes\n",
            "Incorrect:  albeit --- albetit --- albeit\n",
            "Incorrect:  consideration --- consider ate ion --- consider ion\n",
            "Incorrect:  rutherford --- ruther ford --- rutherford\n",
            "Incorrect:  asiatics --- asiatcis --- asiatic\n",
            "Incorrect:  sunnily --- sunn y ly --- sun y ly\n",
            "Incorrect:  umbilical --- umbiblic al --- umbilical\n",
            "Incorrect:  snelling --- snell --- snelling\n",
            "Incorrect:  probabilistic --- prob able ity ic --- prob able ist ic\n",
            "Incorrect:  rivalries --- rivalrie --- rivalry\n",
            "Incorrect:  volumetrically --- volum er ic ly --- volume etr ic al ly\n",
            "Incorrect:  poindexter --- poin detert --- poindexter\n",
            "Incorrect:  waylaying --- way lay n --- way lay\n",
            "Incorrect:  docility --- docility --- docile ity\n",
            "Incorrect:  firmest --- firmest --- firm est\n",
            "Incorrect:  wagering --- wager --- wage er\n",
            "Incorrect:  packard --- pack ard --- packard\n",
            "Incorrect:  combinations --- combine ate ion --- combine ion\n",
            "Incorrect:  anciently --- ancinel ity --- ancient ly\n",
            "Incorrect:  donnish --- donnish --- don ish\n",
            "Incorrect:  incisiveness --- incide ive ness --- in cise ive ness\n",
            "Incorrect:  jeans --- jean --- jeans\n",
            "Incorrect:  athabascan --- athabascan --- athabasca n\n",
            "Incorrect:  alkalis --- alkal ism --- alkal i\n",
            "Incorrect:  addressograph --- address goaph --- address graph\n",
            "Incorrect:  sesquipedalian --- sequipse al ize --- sesqui podo al ian\n",
            "Incorrect:  featherbrained --- feather ream --- feather brain\n",
            "Incorrect:  tonight's --- tonight --- to night\n",
            "Incorrect:  lagrange's --- lagrange --- lag range\n",
            "Incorrect:  arty --- arty --- art y\n",
            "Incorrect:  engulfs --- engulf --- en gulf\n",
            "Incorrect:  phenomenological --- phen mono log ic al --- phenomen log ic al\n",
            "Incorrect:  soignee --- soigene --- soignee\n",
            "Incorrect:  beebread --- beer bead --- bee bread\n",
            "Incorrect:  hendry --- hend y --- hendry\n",
            "Incorrect:  entry --- entry --- enter y\n",
            "Incorrect:  flatlet --- flatelt --- flat let\n",
            "Incorrect:  undue --- undue --- un due\n",
            "Incorrect:  ashtrays --- asthray --- ash tray\n",
            "Incorrect:  northrop --- north rop --- northrop\n",
            "Incorrect:  leash --- leahs --- leash\n",
            "Incorrect:  unseat --- un sea t --- un seat\n",
            "Incorrect:  tenderheartedness --- tender heart ment --- tender heart ness\n",
            "Incorrect:  researches --- re search ess --- re search\n",
            "Incorrect:  thraldom --- thrald mon --- thraldom\n",
            "Incorrect:  unacceptable --- un a cept able --- un accept able\n",
            "Incorrect:  sidings --- siding --- side\n",
            "Incorrect:  circumpolar --- circum ploar er --- circum pole ar\n",
            "Incorrect:  heterosexually --- hetero sex al ly --- hetero sex al y\n",
            "Incorrect:  reprisals --- reprisal --- reprise al\n",
            "Incorrect:  remade --- remade --- re make\n",
            "Incorrect:  halve --- half --- halve\n",
            "Incorrect:  mahouts --- mahouth --- mahout\n",
            "Incorrect:  arsenic --- arsenci --- arsenic\n",
            "Incorrect:  internationale --- inter nation al --- inter nation ale\n",
            "Incorrect:  microbiology --- microbe log y --- micro bio log y\n",
            "Incorrect:  esoterically --- esoteric al ly --- esoteric ly\n",
            "Incorrect:  insidiously --- indisious ly --- insidious ly\n",
            "Incorrect:  kittler --- kittle er --- kittler\n",
            "Incorrect:  sextette --- sexette --- sextette\n",
            "Incorrect:  wicker --- wick er --- wicker\n",
            "Incorrect:  greenwich --- green wick --- green wich\n",
            "Incorrect:  lobs --- lob --- lobe\n",
            "Incorrect:  melodiousness --- melod ious ness --- melod y ious ness\n",
            "Incorrect:  dextrose --- dextrose --- dexter ose\n",
            "Incorrect:  immunoelectrophoresis --- im muneol ety mont --- immun electro phoresis\n",
            "Incorrect:  presbyterianism --- presbytery ian --- presbytery ian ism\n",
            "Incorrect:  arclike --- arclike --- arc like\n",
            "Incorrect:  biddable --- biddable --- bid able\n",
            "Incorrect:  containment --- contain ment --- co tain ment\n",
            "Incorrect:  statutory --- statuorty --- statute ory\n",
            "Incorrect:  messianic --- messine ia --- messiah ic\n",
            "Incorrect:  byzantine --- by zantine --- byzantine\n",
            "Incorrect:  lightning --- light ning --- lightning\n",
            "Incorrect:  penumbras --- pen umbra --- penumbra\n",
            "Incorrect:  steadfast --- stead fast --- steadfast\n",
            "Incorrect:  vaudeville --- vaude ive ly --- vaudeville\n",
            "Incorrect:  antiquated --- antiquate --- antique ate\n",
            "Incorrect:  cardiogram --- cardio graph --- cardio gram\n",
            "Incorrect:  whichsoever --- whicho sever --- which so ever\n",
            "Incorrect:  rheumatoid --- rheumate ion --- rheuma oid\n",
            "Incorrect:  mucilage --- mucil age --- mucilage\n",
            "Incorrect:  hypothalamus --- hypo thalam ize --- hypo thalam us\n",
            "Incorrect:  interdependent --- inter depend ant --- inter de pend ant\n",
            "Incorrect:  booty --- boot y --- booty\n",
            "Incorrect:  venturesomeness --- venture ious ness --- venture some ness\n",
            "Incorrect:  vigorously --- vivor ious ly --- vigor ious ly\n",
            "Incorrect:  osteoporosis --- osteo poo is --- osteo pore sis\n",
            "Incorrect:  funguses --- fungus --- fung us\n",
            "Incorrect:  gondoliers --- gondol y er --- gondol er\n",
            "Incorrect:  verbosity --- verb or ity --- verb ose ity\n",
            "Incorrect:  bengalis --- bengalis --- bengal i\n",
            "Incorrect:  quantitatively --- quant ive  tylle --- quant ive ly\n",
            "Incorrect:  parolees --- parolee --- parole ee\n",
            "Incorrect:  thermoelectric --- therme lect or al --- thermo electr ic\n",
            "Incorrect:  krakow --- brakow --- krakow\n",
            "Incorrect:  scrawniest --- scrawn y est --- scrawny est\n",
            "Incorrect:  amalgamations --- amalgam ion --- amalgam ate ion\n",
            "Incorrect:  strongarm --- strong harm --- strong arm\n",
            "Incorrect:  herein --- herein --- here in\n",
            "Incorrect:  kuwaitis --- kuawiti --- kuwait i\n",
            "Incorrect:  faery --- faer y --- faery\n",
            "Incorrect:  systematical --- system ic ly --- system ic al\n",
            "Incorrect:  provisory --- provide ory --- pro vis ory\n",
            "Incorrect:  longwindedly --- long wide ly --- long wind ly\n",
            "5000 4326   Accuracy: 86.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQQMQR0YJmic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdKFLog_JqLg",
        "colab_type": "code",
        "outputId": "336f3d6a-9970-4788-cf1c-0b931b85a8a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# there are 1045 data in dataset\n",
        "dialogue = \"\"\n",
        "firstLine = None\n",
        "count = -1\n",
        "f = open('All_Data_without_sp.txt', 'r')\n",
        "age = []\n",
        "sex = []\n",
        "X = []\n",
        "y = []\n",
        "for line in f:\n",
        "    if (line == \"<data>\\n\"):\n",
        "        if (count != -1):\n",
        "            X.append(dialogue)\n",
        "        dialogue = \"\"\n",
        "        count += 1\n",
        "        firstLine = None\n",
        "        continue\n",
        "    if (firstLine == None):\n",
        "        firstLine = line\n",
        "        firstSplit = firstLine.split(', ')\n",
        "        #print(firstSplit)\n",
        "        age.append(firstSplit[0])\n",
        "        sex.append(firstSplit[1])\n",
        "        if (firstSplit[2] == \"SLI\\n\"):\n",
        "            y.append(1)\n",
        "        else:\n",
        "            y.append(0)\n",
        "        continue\n",
        "    dialogue = dialogue + \" \" + line\n",
        "X.append(dialogue)\n",
        "print(\"Data Size:\", len(X), len(y))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Size: 1045 1045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu-Qs9ujJ_c6",
        "colab_type": "code",
        "outputId": "bce203e4-be51-4ed6-ee1b-0bf921fd0e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "new_X = []\n",
        "for sentence in X:\n",
        "  new_word = None\n",
        "  for word in sentence.split():\n",
        "    input_data = np.zeros((1, max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
        "    if (word == \"PAUSE1\" or word == \"PAUSE2\" or word == \"PAUSE3\" or word.count(\",\") > 0):\n",
        "      decoded_sequence = word\n",
        "    else:\n",
        "      word = word.lower()\n",
        "      for t, char in enumerate(word):\n",
        "        try:\n",
        "          input_data[0, t, input_token_index[char]] = 1.\n",
        "        except:\n",
        "          #do nothing\n",
        "          #print(\"special_character:\", char)\n",
        "          None\n",
        "      #print(word, \" ==> \", input_data)\n",
        "      decoded_sequence = decode_sequence(input_data)\n",
        "    if (new_word == None):\n",
        "      new_word = decoded_sequence\n",
        "    else:\n",
        "      new_word += \" \" + decoded_sequence\n",
        "  #print(sentence, \" --> \", new_word)\n",
        "  new_X.append(new_word)\n",
        "print(new_X[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "play\n",
            " ball\n",
            " and\n",
            " PAUSE1 he\n",
            " take\n",
            " swim\n",
            " pool\n",
            " and\n",
            " he\n",
            " get\n",
            " it\n",
            " and\n",
            " thank\n",
            " you\n",
            " and\n",
            " welcome\n",
            " and\n",
            " over\n",
            " play\n",
            " swim\n",
            " and\n",
            " go\n",
            " in\n",
            " it\n",
            " and\n",
            " run\n",
            " and\n",
            " walk\n",
            " on\n",
            " knee\n",
            " and\n",
            " her\n",
            " cry\n",
            " PAUSE2 and\n",
            " cry\n",
            " lot\n",
            " and\n",
            " sit\n",
            " on\n",
            " bench\n",
            " and\n",
            " stay\n",
            " here\n",
            " and\n",
            " stay\n",
            " on\n",
            " bench\n",
            " over\n",
            " play\n",
            " play\n",
            " a\n",
            " play\n",
            " and\n",
            " play\n",
            " a\n",
            " play\n",
            " again\n",
            " and\n",
            " play\n",
            " it\n",
            " in\n",
            " water\n",
            " and\n",
            " PAUSE1 it\n",
            " angry\n",
            " and\n",
            " PAUSE1 say\n",
            " PAUSE2 bad\n",
            " and\n",
            " PAUSE1 dad\n",
            " dad y\n",
            " i\n",
            " play\n",
            " in\n",
            " pool\n",
            " and\n",
            " PAUSE1 i\n",
            " get\n",
            " it\n",
            " look\n",
            " and\n",
            " get\n",
            " it\n",
            " and\n",
            " PAUSE1 it\n",
            " in\n",
            " there\n",
            " i\n",
            " get\n",
            " it\n",
            " you\n",
            " get\n",
            " it\n",
            " and\n",
            " PAUSE1 love\n",
            " it\n",
            " he\n",
            " play\n",
            " in\n",
            " sand\n",
            " and\n",
            " play\n",
            " make\n",
            " castle\n",
            " and\n",
            " dump\n",
            " it\n",
            " dump\n",
            " it\n",
            " oh\n",
            " no\n",
            " oops\n",
            " sorry\n",
            " and\n",
            " cry\n",
            " uhoh\n",
            " they\n",
            " go\n",
            " picnic\n",
            " and\n",
            " eat\n",
            " and\n",
            " they\n",
            " drink\n",
            " juice\n",
            " and\n",
            " hungry\n",
            " and\n",
            " PAUSE1 that\n",
            " mann\n",
            " sick\n",
            " sick\n",
            " and\n",
            " PAUSE1 a\n",
            " bunny\n",
            " PAUSE1 his\n",
            " PAUSE1 tummy\n",
            " hurt\n",
            " and\n",
            " turn\n",
            " that\n",
            " and\n",
            " PAUSE1 go\n",
            " doctor\n",
            " house\n",
            " doctor\n",
            " all\n",
            " done\n",
            " her\n",
            " play\n",
            " and\n",
            " PAUSE1 hold\n",
            " that\n",
            " up\n",
            " high er\n",
            " tie\n",
            " it\n",
            " up\n",
            " and\n",
            " up\n",
            " high er\n",
            " and\n",
            " up\n",
            " cloud\n",
            " angry\n",
            " and\n",
            " PAUSE1 mann\n",
            " mann\n",
            " balloon\n",
            " on\n",
            " it\n",
            " PAUSE1 him\n",
            " take\n",
            " one\n",
            " and\n",
            " and\n",
            " bring\n",
            " doctor\n",
            " here\n",
            " and\n",
            " check\n",
            " and\n",
            " talk\n",
            " and\n",
            " money\n",
            " and\n",
            " balloon\n",
            " and\n",
            " two\n",
            " balloon\n",
            " and\n",
            " over\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS3QUqPqnnBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write to new file\n",
        "f2 = open('All_Data_morphemes.txt', 'w')\n",
        "for i in range(len(new_X)):\n",
        "  f2.write(\"<data>\")\n",
        "  f2.write(\"\\n\")\n",
        "  if (y[i] == 1):\n",
        "    f2.write(\"SLI\")\n",
        "  else:\n",
        "    f2.write(\"TD\")\n",
        "  f2.write(\"\\n\")\n",
        "  f2.write(new_X[i])\n",
        "  f2.write(\"\\n\")\n",
        "f2.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}